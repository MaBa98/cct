{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CCTeu Pricing Model - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive exploratory data analysis for the CCTeu pricing model.\n",
    "\n",
    "## Analysis Structure:\n",
    "1. Data Loading and Initial Inspection\n",
    "2. Time Series Analysis\n",
    "3. Correlation Analysis\n",
    "4. Volatility Analysis\n",
    "5. Relative Value Analysis\n",
    "6. Feature Engineering Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed features\n",
    "try:\n",
    "    features_df = pd.read_csv('../data/processed/features.csv', index_col=0, parse_dates=True)\n",
    "    print(f\"Features dataset shape: {features_df.shape}\")\n",
    "    print(f\"Date range: {features_df.index.min()} to {features_df.index.max()}\")\n",
    "    print(f\"Missing values: {features_df.isnull().sum().sum()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Features file not found. Please run main.py first.\")\n",
    "    features_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Basic statistics\n",
    "    display(features_df.describe())\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData types:\")\n",
    "    print(features_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Time Series Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Plot price levels\n",
    "    price_cols = [col for col in features_df.columns if col.startswith('price_')]\n",
    "    \n",
    "    if price_cols:\n",
    "        fig = make_subplots(rows=2, cols=1, \n",
    "                           subplot_titles=('CCTeu Price Levels', 'Benchmark Instruments'),\n",
    "                           vertical_spacing=0.1)\n",
    "        \n",
    "        # CCTeu prices\n",
    "        ccteu_prices = [col for col in price_cols if 'IT000' in col]\n",
    "        for col in ccteu_prices:\n",
    "            fig.add_trace(go.Scatter(x=features_df.index, y=features_df[col], \n",
    "                                   name=col.replace('price_', ''), mode='lines'),\n",
    "                         row=1, col=1)\n",
    "        \n",
    "        # Benchmark prices\n",
    "        benchmark_prices = [col for col in price_cols if 'IT000' not in col]\n",
    "        for col in benchmark_prices:\n",
    "            fig.add_trace(go.Scatter(x=features_df.index, y=features_df[col], \n",
    "                                   name=col.replace('price_', ''), mode='lines'),\n",
    "                         row=2, col=1)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"Historical Price Analysis\")\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Plot returns\n",
    "    return_cols = [col for col in features_df.columns if col.startswith('ret_')]\n",
    "    \n",
    "    if return_cols:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Returns time series\n",
    "        features_df[return_cols].plot(ax=axes[0,0], alpha=0.7)\n",
    "        axes[0,0].set_title('Returns Time Series')\n",
    "        axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Returns distribution\n",
    "        for col in return_cols[:5]:  # Limit to first 5 for readability\n",
    "            axes[0,1].hist(features_df[col].dropna(), alpha=0.6, bins=50, label=col)\n",
    "        axes[0,1].set_title('Returns Distribution')\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Rolling volatility\n",
    "        rolling_vol = features_df[return_cols].rolling(window=20).std() * np.sqrt(252)\n",
    "        rolling_vol.plot(ax=axes[1,0], alpha=0.7)\n",
    "        axes[1,0].set_title('Rolling Volatility (20-day)')\n",
    "        axes[1,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # QQ plot for normality check\n",
    "        if len(return_cols) > 0:\n",
    "            stats.probplot(features_df[return_cols[0]].dropna(), dist=\"norm\", plot=axes[1,1])\n",
    "            axes[1,1].set_title(f'Q-Q Plot: {return_cols[0]}')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Correlation matrix for returns\n",
    "    return_cols = [col for col in features_df.columns if col.startswith('ret_')]\n",
    "    \n",
    "    if return_cols:\n",
    "        corr_matrix = features_df[return_cols].corr()\n",
    "        \n",
    "        plt.figure(figsize=(12, 10))\n",
    "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "        sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "        plt.title('Returns Correlation Matrix')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print highest correlations\n",
    "        print(\"\\nHighest correlations (excluding self-correlation):\")\n",
    "        corr_unstack = corr_matrix.unstack()\n",
    "        corr_unstack = corr_unstack[corr_unstack < 0.99].sort_values(ascending=False)\n",
    "        print(corr_unstack.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Volatility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Volatility statistics\n",
    "    return_cols = [col for col in features_df.columns if col.startswith('ret_')]\n",
    "    \n",
    "    if return_cols:\n",
    "        vol_stats = pd.DataFrame()\n",
    "        \n",
    "        for col in return_cols:\n",
    "            returns = features_df[col].dropna()\n",
    "            vol_stats.loc[col, 'Ann_Volatility'] = returns.std() * np.sqrt(252)\n",
    "            vol_stats.loc[col, 'Skewness'] = returns.skew()\n",
    "            vol_stats.loc[col, 'Kurtosis'] = returns.kurtosis()\n",
    "            vol_stats.loc[col, 'Max_Drawdown'] = (returns.cumsum() - returns.cumsum().cummax()).min()\n",
    "            vol_stats.loc[col, 'VaR_95'] = returns.quantile(0.05)\n",
    "            vol_stats.loc[col, 'CVaR_95'] = returns[returns <= returns.quantile(0.05)].mean()\n",
    "        \n",
    "        display(vol_stats.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Augmented Dickey-Fuller test for stationarity\n",
    "    def adf_test(series, name):\n",
    "        result = adfuller(series.dropna())\n",
    "        return {\n",
    "            'Series': name,\n",
    "            'ADF_Statistic': result[0],\n",
    "            'p_value': result[1],\n",
    "            'Critical_Values_1%': result[4]['1%'],\n",
    "            'Critical_Values_5%': result[4]['5%'],\n",
    "            'Stationary': result[1] < 0.05\n",
    "        }\n",
    "    \n",
    "    # Test all numeric columns\n",
    "    numeric_cols = features_df.select_dtypes(include=[np.number]).columns\n",
    "    stationarity_results = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if features_df[col].dropna().shape[0] > 10:  # Minimum observations\n",
    "            stationarity_results.append(adf_test(features_df[col], col))\n",
    "    \n",
    "    stationarity_df = pd.DataFrame(stationarity_results)\n",
    "    display(stationarity_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Relative Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rv_signals = pd.read_csv('../data/processed/relative_value_signals.csv', index_col=0, parse_dates=True)\n",
    "    \n",
    "    if not rv_signals.empty:\n",
    "        # Plot relative value signals\n",
    "        fig = make_subplots(rows=2, cols=1, \n",
    "                           subplot_titles=('Relative Value Z-Scores', 'Current Positioning'),\n",
    "                           vertical_spacing=0.15)\n",
    "        \n",
    "        # Time series of RV signals\n",
    "        for col in rv_signals.columns:\n",
    "            fig.add_trace(go.Scatter(x=rv_signals.index, y=rv_signals[col], \n",
    "                                   name=col, mode='lines'), row=1, col=1)\n",
    "        \n",
    "        # Current positioning (bar chart)\n",
    "        current_signals = rv_signals.iloc[-1]\n",
    "        fig.add_trace(go.Bar(x=current_signals.index, y=current_signals.values, \n",
    "                           name='Current Z-Score'), row=2, col=1)\n",
    "        \n",
    "        fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", row=1, col=1)\n",
    "        fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\", row=2, col=1)\n",
    "        \n",
    "        fig.update_layout(height=800, title_text=\"Relative Value Analysis\")\n",
    "        fig.show()\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(\"\\nRelative Value Signal Statistics:\")\n",
    "        display(rv_signals.describe())\n",
    "        \n",
    "        print(\"\\nCurrent Signals (Most Recent):\")\n",
    "        current_sorted = current_signals.sort_values()\n",
    "        print(f\"Most Undervalued: {current_sorted.index[0]} (Z-Score: {current_sorted.iloc[0]:.2f})\")\n",
    "        print(f\"Most Overvalued: {current_sorted.index[-1]} (Z-Score: {current_sorted.iloc[-1]:.2f})\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(\"Relative value signals file not found. Please run main.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    # Z-score analysis\n",
    "    z_score_cols = [col for col in features_df.columns if col.startswith('z_')]\n",
    "    \n",
    "    if z_score_cols:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Z-scores time series\n",
    "        features_df[z_score_cols].plot(ax=axes[0,0], alpha=0.7)\n",
    "        axes[0,0].set_title('Z-Scores Time Series')\n",
    "        axes[0,0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[0,0].axhline(y=2, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0,0].axhline(y=-2, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Z-scores distribution\n",
    "        for col in z_score_cols:\n",
    "            axes[0,1].hist(features_df[col].dropna(), alpha=0.6, bins=50, label=col)\n",
    "        axes[0,1].set_title('Z-Scores Distribution')\n",
    "        axes[0,1].axvline(x=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Spread analysis\n",
    "        spread_cols = [col for col in features_df.columns if 'spread' in col]\n",
    "        if spread_cols:\n",
    "            features_df[spread_cols].plot(ax=axes[1,0], alpha=0.7)\n",
    "            axes[1,0].set_title('Spread Analysis')\n",
    "            axes[1,0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        # Feature importance proxy (correlation with first CCTeu return)\n",
    "        ccteu_ret_cols = [col for col in features_df.columns if col.startswith('ret_') and 'IT000' in col]\n",
    "        if ccteu_ret_cols:\n",
    "            target_col = ccteu_ret_cols[0]\n",
    "            feature_cols = [col for col in features_df.columns if col.startswith(('ret_', 'z_')) and col != target_col]\n",
    "            \n",
    "            correlations = []\n",
    "            for col in feature_cols:\n",
    "                corr = features_df[target_col].corr(features_df[col])\n",
    "                correlations.append({'Feature': col, 'Correlation': abs(corr)})\n",
    "            \n",
    "            corr_df = pd.DataFrame(correlations).sort_values('Correlation', ascending=True)\n",
    "            \n",
    "            axes[1,1].barh(range(len(corr_df)), corr_df['Correlation'])\n",
    "            axes[1,1].set_yticks(range(len(corr_df)))\n",
    "            axes[1,1].set_yticklabels(corr_df['Feature'])\n",
    "            axes[1,1].set_title(f'Feature Importance (Correlation with {target_col})')\n",
    "            axes[1,1].set_xlabel('Absolute Correlation')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"EXPLORATORY DATA ANALYSIS - SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Data quality summary\n",
    "    print(f\"\\n1. DATA QUALITY:\")\n",
    "    print(f\"   - Dataset shape: {features_df.shape}\")\n",
    "    print(f\"   - Date range: {features_df.index.min().strftime('%Y-%m-%d')} to {features_df.index.max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   - Missing values: {features_df.isnull().sum().sum()}/{features_df.size} ({features_df.isnull().sum().sum()/features_df.size*100:.2f}%)\")\n",
    "    \n",
    "    # Returns analysis\n",
    "    return_cols = [col for col in features_df.columns if col.startswith('ret_')]\n",
    "    if return_cols:\n",
    "        print(f\"\\n2. RETURNS ANALYSIS:\")\n",
    "        returns_stats = features_df[return_cols].describe()\n",
    "        print(f\"   - Average daily return: {returns_stats.loc['mean'].mean():.6f}\")\n",
    "        print(f\"   - Average daily volatility: {returns_stats.loc['std'].mean():.6f}\")\n",
    "        print(f\"   - Annualized volatility range: {returns_stats.loc['std'].min()*np.sqrt(252):.2f}% - {returns_stats.loc['std'].max()*np.sqrt(252):.2f}%\")\n",
    "    \n",
    "    # Correlation insights\n",
    "    if len(return_cols) > 1:\n",
    "        corr_matrix = features_df[return_cols].corr()\n",
    "        avg_corr = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].mean()\n",
    "        print(f\"\\n3. CORRELATION ANALYSIS:\")\n",
    "        print(f\"   - Average pairwise correlation: {avg_corr:.3f}\")\n",
    "        print(f\"   - Highest correlation: {corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].max():.3f}\")\n",
    "        print(f\"   - Lowest correlation: {corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)].min():.3f}\")\n",
    "    \n",
    "    # Feature engineering validation\n",
    "    z_score_cols = [col for col in features_df.columns if col.startswith('z_')]\n",
    "    if z_score_cols:\n",
    "        print(f\"\\n4. FEATURE ENGINEERING:\")\n",
    "        print(f\"   - Z-score features: {len(z_score_cols)}\")\n",
    "        extreme_z_scores = (features_df[z_score_cols].abs() > 2).sum().sum()\n",
    "        total_z_obs = features_df[z_score_cols].count().sum()\n",
    "        print(f\"   - Extreme z-scores (|z| > 2): {extreme_z_scores}/{total_z_obs} ({extreme_z_scores/total_z_obs*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Analysis complete. Review plots above for detailed insights.\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
